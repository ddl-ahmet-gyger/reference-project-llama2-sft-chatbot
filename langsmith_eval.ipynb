{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09b4f4-97fb-430c-bb55-8a40008d0f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show langchain --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d584685e-c422-4ad8-9268-ae27de2e9b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "# To Avoid the Error on Jupyter Notebook (RuntimeError: This Event Loop Is Already Running)\n",
    "# Patch Asyncio To Allow Nested Event Loops\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09997cd-aad9-4500-a787-1430635a46cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f89186-78ae-4445-8e83-5a77248d3d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the LangSmith Client and LLM\n",
    "\n",
    "client = Client()\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015e5c5-8a18-4718-bcbd-3ae7e6fa28e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create a Dataset (Only Inputs, No Output)\n",
    "\n",
    "example_inputs = [\n",
    "    \"Complete the following Python function that computes the factorial of a number: \\ndef factorial(n):\",\n",
    "    \"Summarize the following paragraph: \\'Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \\\"intelligent agents\\\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\",\n",
    "    \"Generate a short poem about the beauty of nature.\",\n",
    "    \"You are in a dark room with a single door. There's a switch next to the door. What do you do?\",\n",
    "]\n",
    "\n",
    "dataset_name = \"Misc tasks dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Few diverse tasks\",\n",
    ")\n",
    "\n",
    "for input_prompt in example_inputs:\n",
    "    # Each example must be unique and have inputs defined.\n",
    "    # Outputs are optional\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs=None,\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45e4e6-f1ea-45ca-9652-02b8ec33dda5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Evaluate Datasets with LLM\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can specify an evaluator by name/enum.\n",
    "        # In this case, the default criterion is \"helpfulness\"\n",
    "        \"criteria\",\n",
    "        # Or you can configure the evaluator\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria(\"misogyny\"),\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\n",
    "                \"short_informative\": \"Are the answers short and informative? \"\n",
    "                \"Respond Y if they are, N if they're not short and informative.\"\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae83a7-9d5f-4535-a7f1-7c5dd079d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create a Dataset From a List of Examples (Key-Value Pairs)\n",
    "\n",
    "example_inputs = [\n",
    "    (\"Complete the following Python function that computes the factorial of a number: \\ndef factorial(n):\", \"def factorial(n): \\n if n == 0: \\n return 1 \\n else: \\n return n * factorial(n-1)\"),\n",
    "    (\"Summarize the following paragraph: 'Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \\\"intelligent agents\\\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\", \n",
    "     \"AI is machine intelligence as opposed to natural human or animal intelligence. It's defined as the study of devices that act intelligently to achieve their goals.\"),\n",
    "    (\"You are in a dark room with a single door. There's a switch next to the door. What do you do?\", \"I would flip the switch to see if it turns on a light.\"),\n",
    "    (\n",
    "        \"Convert the following statement into a question: 'The Eiffel Tower is located in Paris.'\",\n",
    "        \"Where is the Eiffel Tower located?\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset_name = \"Tasks and Answers\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions and answers about diverse tasks\",\n",
    ")\n",
    "\n",
    "for input_prompt, output_answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs={\"answer\": output_answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228d1cd-e018-4efb-aefc-590cae7fc087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Create a Dataset From a Dataframe\n",
    "\n",
    "# Create a Dataframe\n",
    "df_dataset = pd.DataFrame(example_inputs, columns=[\"Question\", \"Answer\"])\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdb81d-f8e9-4df8-862a-d8fb2fcd7c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_keys = [\"Question\"]\n",
    "output_keys = [\"Answer\"]\n",
    "\n",
    "# Create Dataset\n",
    "\n",
    "dataset = client.upload_dataframe(\n",
    "    df=df_dataset,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"Tasks Dataframe Dataset\",\n",
    "    description=\"Dataset created from a dataframe\",\n",
    "    data_type=\"kv\",  # The default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5cc0a-a837-4e76-8a69-9dc719a33760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Create a Dataset From a CSV File\n",
    "\n",
    "# Save the Dataframe as a CSV File\n",
    "\n",
    "csv_path = \"./data/dataset.csv\"\n",
    "df_dataset.to_csv(csv_path, index=False)\n",
    "\n",
    "# Create Dataset\n",
    "\n",
    "dataset = client.upload_csv(\n",
    "    csv_file=csv_path,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"Tasks CSV Dataset\",\n",
    "    description=\"Dataset created from a CSV file\",\n",
    "    data_type=\"kv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b358d-165b-4a58-90c6-e776b072fdd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Evaluate Datasets That Contain Labels\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"qa\",  # correctness: right or wrong\n",
    "        \"context_qa\",  # refer to example outputs\n",
    "        \"cot_qa\",  # context_qa + reasoning\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Tasks and Answers\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e962128-abf0-4472-a628-8f60c2f06c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Evaluate Datasets with customized criteria\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            {\n",
    "                \"helpfulness\": (\n",
    "                    \"Is this submission helpful to the user,\"\n",
    "                    \" taking into account the correct reference answer?\"\n",
    "                )\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Tasks CSV Dataset\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba08514-5ddc-494a-a615-b295469fcd9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Evaluate Datasets without Labels with a custom evaluator to compute perplexity\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from evaluate import load\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "import time\n",
    "import textstat\n",
    "\n",
    "class FKEvaluator(RunEvaluator):\n",
    "    def __init__(self):\n",
    "        self.metric_fn = textstat.flesch_kincaid_grade\n",
    "\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        prediction = run.outputs['generations'][0][0]['text']\n",
    "        return EvaluationResult(key=\"flesch_kincaid_grade\", score=self.metric_fn(prediction))\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\"creativity_correct\": \"Is this answers creative and correct?\"}\n",
    "        ),\n",
    "        # We provide some simple default criteria like \"conciseness\" you can use as well\n",
    "        RunEvalConfig.Criteria(\"conciseness\"),\n",
    "    ],\n",
    "    custom_evaluators = [FKEvaluator()]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Misc tasks dataset\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea84638-9aef-4cf8-9b1d-547a69dfaaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Evaluate Datasets Based on Cosine Distance Criteria\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        \"embedding_distance\",\n",
    "        # Or to customize the embeddings:\n",
    "        # Requires 'pip install sentence_transformers'\n",
    "        # RunEvalConfig.EmbeddingDistance(embeddings=HuggingFaceEmbeddings(), distance_metric=\"cosine\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Tasks Dataframe Dataset\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253c05b-97db-4bea-ae22-431cf4efbf15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Evaluate Datasets Based on String Distance Criteria\n",
    "# Jaro-Winkler Similarity Distance: 0 = Exact Match, 1 = No Similarity\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        \"string_distance\",\n",
    "        # Or to customize the distance metric:\n",
    "        # RunEvalConfig.StringDistance(distance=\"levenshtein\", normalize_score=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Tasks Dataframe Dataset\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679198b-3fd9-4e28-9356-e37dce862c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
