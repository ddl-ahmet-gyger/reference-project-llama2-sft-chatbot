{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5730c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet -U langchain\n",
    "! pip install --quiet -U langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea88208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# -- Bitsandbytes parameters --\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Chat model\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "pipe_llama7b_chat = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=300) # set device to run inference on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Fine-tuned model\n",
    "model_path ='subirmansukhani/llama-2-7b-miniguanaco'\n",
    "\n",
    "model_loaded = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                    quantization_config=bnb_config,\n",
    "                                                    cache_dir=\"/mnt/artifacts/llama2-model-cache/\",\n",
    "                                                    device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156a173e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"<s>[INST] What should i do in Paris? [/INST] Paris is a beautiful city with a rich history and culture. There are many things you can do in Paris, depending on your interests and preferences. Here are a few suggestions:\\n\\n1. Visit famous landmarks: Paris is home to many iconic landmarks, including the Eiffel Tower, Notre-Dame Cathedral, and the Arc de Triomphe. You can take a guided tour of these landmarks or visit them on your own.\\n2. Explore museums: Paris has a number of world-class museums, including the Louvre, the Mus√©e d'Orsay, and the Centre Pompidou. These museums house some of the most famous works of art in the world, including the Mona Lisa and Van Gogh's Starry Night.\\n3. Take a river cruise: A river cruise along the Seine is\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe_llama7b_chat_ft = pipeline(task=\"text-generation\", model=model_loaded, tokenizer=tokenizer, max_length=200) # Set device to run inference on GPU\n",
    "\n",
    "prompt = \"What should i do in Paris?\"\n",
    "text = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "result = pipe_llama7b_chat_ft(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Optional\n",
    "from langchain.evaluation import StringEvaluator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import openai_functions\n",
    "\n",
    "eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an impartial grader tasked with measuring the accuracy of extracted entity relations.\"),\n",
    "        (\"human\", \"Please evaluate the following data:\\n\\n\"\n",
    "         \"<INPUT>\\n{input}</INPUT>\\n\"\n",
    "         \"<PREDICTED>\\n{prediction}</PREDICTED>\\n\"\n",
    "         \"<GROUND_TRUTH>\\n{reference}</GROUND_TRUTH>\\n\\n\"\n",
    "         \"Please save your reasoning and grading by calling the commit_grade function.\"\n",
    "         \" First, enumerate all factual discrepancies in the predicted triplets relative to the ground truth.\"\n",
    "         \" Finally, score the prediction on a scale out of 100, taking into account factuality and\"\n",
    "         \" correctness according to the ground truth.\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "commit_grade_schema = {\n",
    "    \"name\": \"commit_grade\",\n",
    "    \"description\": \"Commits a grade with reasoning.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"commit_grade_parameters\",\n",
    "        \"description\": \"Parameters for the commit_grade function.\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"mistakes\": {\n",
    "                \"title\": \"discrepancies\",\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any discrepencies between the predicted and ground truth.\"\n",
    "            },\n",
    "            \"reasoning\": {\n",
    "                \"title\": \"reasoning\",\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The explanation or logic behind the final grade.\"\n",
    "            },\n",
    "            \"grade\": {\n",
    "                \"title\": \"grade\",\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The numerical value representing the grade.\",\n",
    "                \"minimum\": 0,\n",
    "                \"maximum\": 100\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"reasoning\", \"grade\", \"mistakes\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "def normalize_grade(func_args: str) -> dict:\n",
    "    args = json.loads(func_args)\n",
    "    return {\n",
    "        \"reasoning\": (args.get(\"reasoning\", \"\") + \"\\n\\n\" + args.get(\"discrepancies\", \"\")).strip(),\n",
    "        \"score\": args.get(\"grade\", 0) / 100,\n",
    "    }\n",
    "\n",
    "eval_chain = (\n",
    "    eval_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4\", temperature=0).bind(functions=[commit_grade_schema])\n",
    "    | openai_functions.OutputFunctionsParser()\n",
    "    | normalize_grade\n",
    ")\n",
    "\n",
    "class EvaluateTriplets(StringEvaluator):\n",
    "    \"\"\"Evaluate the triplets of a predicted string.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        *,\n",
    "        prediction: str,\n",
    "        reference: Optional[str] = None,\n",
    "        input: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        callbacks = kwargs.pop(\"callbacks\", None)\n",
    "        return eval_chain.invoke(\n",
    "            {\"prediction\": prediction, \"reference\": reference, \"input\": input},\n",
    "            {\"callbacks\": callbacks},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d369149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "client = Client()\n",
    "# Note that \"sentence\" is the key in the test dataset\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n### Input:{sentence}\\n\\n[/INST]\\n\"\n",
    ").partial(system_message=system_prompt)\n",
    "\n",
    "from langchain.smith import RunEvalConfig\n",
    "config = RunEvalConfig(\n",
    "    custom_evaluators=[EvaluateTriplets()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61447179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat LLM\n",
    "llama_llm_chat = HuggingFacePipeline(pipeline=pipe_llama7b_chat)\n",
    "llama_chain_chat = prompt | llama_llm_chat\n",
    "results = await client.arun_on_dataset(validation_dataset_name, llama_chain_chat, evaluation=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74215142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat LLM w/ FT\n",
    "llama_llm_chat_ft = HuggingFacePipeline(pipeline=pipe_llama7b_chat_ft)\n",
    "llama_chain_chat_ft = prompt | llama_llm_chat_ft\n",
    "results = await client.arun_on_dataset(validation_dataset_name, llama_chain_chat_ft, evaluation=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
